# Music Interaction Design

*by Guillermo Montecinos, NYU ITP Spring 2019*

This document and the contents stored in this repo corresponds to [Music Interaction Design](https://luisaph.github.io/music-interaction-design-spring-2019/) class taught by Luisa Pereira, at NYU ITP during the 2019 Spring term.

## Week 1

### Music Interaction Exercise
We were expected to design a music interaction based on one musical piece and an oblique strategy. The song I chose is [Sulky](https://www.youtube.com/watch?v=8E-Go310oe4) by the Argentinian musician Gustavo Cerati. Sulky is a 4:18 minutes electro-rock [Chacarera](https://en.wikipedia.org/wiki/Chacarera) –a folk music and dance from the countryside of Río de la Plata zone– written in 6/8 and a tempo of 115 bpm.

This is a song built from samples of typical instruments as Legüero bass drum and guitar, piano and a bunch of synths and unrecognizable samples that conform a dreamlike experience of traveling through a dark and open flat land.

In terms of this exercise, the oblique strategy got was: "Look closely at the most embarrassing details and amplify them". Responding to that, I think this song seeks to prepare the listener through a gentle and dreamlike atmosphere to face –probably– darkness and doubt of life. Darkness and dark feelings are constantly in the back of our heads and we usually are not proud off it. Actually many people avoid to talk and even recognize their darkness. But embracing darkness can make us look inside. So, in this experiment I'd like to design an interaction that –through Sulky– would invite one person to travel through his/her emotions being able to go back and face darkness any time it is needed.

<p align="center">
  <img src="https://github.com/guillemontecinos/itp_spring_2019_music_interaction_design/blob/master/assets/week1_diagram1.jpg" align="middle" width="70%">
</p>

The interaction –for only one person– consists in a dark room with one chair, three screens (one in front of the chair displaying visuals that evoke comfortable feelings and one at each side of it that evoke uncomfortable feelings) and a quadraphonic sound system. The person will be invited to travel through a reconstruction of the song in which the sound layers will be spatialized and synchronized to create a 3D immersion of the listener in the world of light and darkness created by Cerati. As the song plays the listener will be exposed to comfortable visuals displayed in the front screen, but will be tempted to look to his/her side to the uncomfortable visuals that –as our own darkness– will be constantly calling their attention. When the user look to any of the side screens –which means is paying attention to the darkness– the uncomfortable/dark/weird part of the song will be built in real time (played) by the system. For this purpose, the user's head will be tracked with a kinetic sensor placed behind the chair.

<p align="center">
  <img src="https://github.com/guillemontecinos/itp_spring_2019_music_interaction_design/blob/master/assets/week1_diagram2.jpg" align="middle" width="70%">
</p>

### Final Project Iteration #1

For this project I'm interested in designing an interaction that through music can explore concepts as **identity, community, oppression and migration**, and how these concepts are understood in the post-globalization world were hateful speeches delivered against under represented and discriminated communities have rose. With a southern perspective I'm concerned in engaging territorial and musical communitarian memory, and how that memory can crop up when identity and community are threatened. As well through this interaction I want to challenge the role of technology as a medium used to create interactive installations or applications.

Regarding the kind of this interaction I haven't addressed the final format it will take, but there are some lights. According to [Towards a Dimension Space for Musical Devices](http://lizbeck.net/pdf/nime2005_192.pdf) musical interactions can be parametrized in 7 dimensions that can be plotted in a dimension space chart, these are role of sound, required expertise, musical control, degrees of freedom, feedback modalities, inter-actors and distribution in space. Despite there is no clarity of how will the interaction look like there is certainty that the number of inter-actors can be high because it will be a community-based interaction. As well, the role of sound has to be expressive to engage with community imaginary, and distribution in space –wether physical or virtual– has to be distributed and decentralized. Finally, as this interaction will seek to engage with primal identity aspects of the community there will expected no expertise from the users.

Some references as [Voluspa Jarpa](http://ismorbo.com/voluspa-jarpa-lleva-la-historia-oculta-de-latinoamerica-y-la-cia-al-matucana-100/)

## Week 2 - Collective orchestra
### Final project Iteration #2
For this iteration of the project I'd like to keep the eye on the concept of community and imagine an interaction around it. According with [Wikipedia](https://en.wikipedia.org/wiki/Community) *A community is a small or large social unit (a group of living things) that has something in common, such as norms, religion, values, or identity* – and usually a sense of territoriality and a common memory as well. The reason why we humans live in communities is because –ideally– together we can make more and better than separated.

Then, a community-based –or communitarian– music interaction can be one that expresses the best of it only when there is an engaged community interacting with it. An interesting goal to approach through this is to connect people around a musical interaction even if they don't know each other –or if they don't have a common memory– and make them realize that the sound landscape they are experiencing exists only because they are they are part of that interaction. As a draft title this interaction can be called *Collective Orchestra* even may not sound as a classic orchestra.

Regarding the above, this interaction can be experienced by any person from any cultural background, so there are no restrictions for play testing.

### Musical User Path
During this iteration I approached the idea of an interactive installation –I'm not quite sure if physical, virtual or maybe both– where music is created collectively. Depending on how many users are interacting in the space the music construction will vary. In this regard, music parameters may be organized hierarchically and my follow this pattern:

* Rhythm
* Melody
* Harmony

The above means that we can think for this instance that rhythm is the base of music, so as soon as a person appears rhythm will start playing a kick drum. As more people join the interaction more instruments will start to play and the performance/interaction will vary. As we can see in the next video, kick is represented by the apparition of the first person, whilst lead synth sequencer and bass synth (sequencer) represent the interaction of a 2nd and 3rd users. This is just a conceptual representation of the importance of collective engage for this interaction.

<p align="center">
  <a href="https://vimeo.com/317072294">
    <img src="https://github.com/guillemontecinos/itp_spring_2019_music_interaction_design/blob/master/assets/week2_user_path_cover.png" align="middle" width="70%">
  </a>
</p>

As a long term design goal, the idea is every inter-actor can control certain parameters of they *musical representation* as timber, melody sequence, eq, etc. As well, I would like to allow one –not sure if any or just one– user to control beat and signature.

### Aural Mood Board
As an aural mood board I have selected some projects/bands/songs/pieces that inspire me for the design of this project:

* [Organelle's](https://www.youtube.com/watch?v=4MA8jQ4lfq4&t=100) (Critter & Guitari) synthesis timber
* [Thom Yorke's mood](https://www.youtube.com/watch?v=BTZl9KMjbrU)
* [Bonobo's live mood](https://www.youtube.com/watch?v=fQAFfeQNpGc&t=20)
* [Format 3](https://vimeo.com/259987819#t=0m45s) (by [FOO/SKOU](http://fooskou.tumblr.com/)) sound synthesis aspect and maybe their installation style
* [Pierre Schaeffer](https://www.youtube.com/watch?v=CTf0yE15zzI) Études de bruits's (1948) manipulation of sound
* [Bombo Legüero's](https://www.youtube.com/watch?v=jo6E56_eBbE) depth
* [Sound of the forest](https://www.youtube.com/watch?v=OdIJ2x3nxzQ) because I like and makes me feel good

## Week 3 - One person interaction
During this week iteration I explored how a one person interaction would be, regarding on the idea that first person can represent rhythm. The original goal was to allow an inter-actor to control play/pause of an Ableton Live arrangement via being or not in a physical space, as well as control tempo and time signature with his/her hands. For this purpose, user's pose was expected to detect using the PoseNet (ML) model running on the Runnway app.

<p align="center">
  <img src="https://github.com/guillemontecinos/itp_spring_2019_music_interaction_design/blob/master/assets/week3_diagram.jpg" align="middle" width="70%">
</p>

The original scheme designed was that after detecting the pose with PoseNet, that information had to be sent to Max/Msp via OSC to be interpreted and converted to MIDI and sent to Ableton Live to control the aforementioned parameters. Due to some protocol issues I couldn't make Runway work in order to detect poses because the message wasn't received at Max (now I know that Max should send an *start* message to Runways OSC server to begin communication, so I'll implement it in further iterations), so I decided to work with a ml5.js implementation of PoseNet which sends the data via Socket/OSC to Max.

<p align="center">
  <img src="https://github.com/guillemontecinos/itp_spring_2019_music_interaction_design/blob/master/assets/week3_maxpat.png" align="middle" width="70%">
</p>

In the above patch, `udpreceive 7500` is listening to the port 7500 for OSC messages. The information sent from the jS sketch contains the vertical position of the left hand in a range of 0 - 127, and the amount of poses detected. The first is passed to `ctlout` –which sends MIDI control messages– while the second is used as a decision variable to play or stop the track, controlled via MIDI by the midinotes #27 and #28 –sent by `noteout`.

During the process as well I realized that Ableton Live's time signature can't be modified remotely by a MIDI mapping –as I could do with tempo and play/stop– because there is not a routing option to time signature values. So I had to rule this interaction option. As it can be seen when the MIDI Mapping menu is opened, *CC 20* to *Song Tempo*, *Note D#0* is connected to *Stop* and *Note E0* is connected to *Start*. At the same time, it can be noticed that the aforementioned buttons are blue-painted, whilst the time signature has no color because can't be mapped.

<p align="center">
  <img src="https://github.com/guillemontecinos/itp_spring_2019_music_interaction_design/blob/master/assets/week3_midimap.png" align="middle" width="70%">
</p>

Finally, regarding the assignment of *make music with chords progressions* I just explored some standard progressions but didn't create an actual composition:

**Major**
* I - IV - V
* I - IIm - V
* I - VIm - IIm - V
* I IIIm - VIm - IIm - V

**Minor**
* Im - VIm - Vm
* Im - IIdim - V (harmonic)
* Im - Vidim - IIm - V (melodic)

But, for the interaction sample I used a basic Major chord progression played in piano as can be seen as follows.

<p align="center">
  <a href="https://vimeo.com/318212799">
    <img src="https://github.com/guillemontecinos/itp_spring_2019_music_interaction_design/blob/master/assets/week3_interaction1.png" align="middle" width="70%">
  </a>
</p>

## Week 5 (& week 4) - Reshaping concept
During last two weeks I struggled with the idea of designing an interaction that doesn't really have a powerful meaning to me. During the previous iterations I have tried to build an idea from the concept of community and collaboration, and how they are meaningful when people gather in a space by creating music. But despite these concepts are still too powerful to me, the interaction I'm imagining from them in this creative process aren't strong enough as I would like. In the class Recurring Concepts in Art we had as Tony Martin as a guest last week, an interactive artist who 50 years ago created an exhibition called [Game Room](https://www.eai.org/supporting-documents/686) in which people walking in a room triggered sound and lights with their movements. Seeing his artwork made me think that I have not a strong justification to use the same technique that has been used for almost 50 years, unless I have a strong concept that make it new.

Due to the above I spent almost two weeks thinking about a question from which I can build a design that made sense to work in. And I remembered a conversation I had with Francisca Cabrera –a Chilean Special Educator– about being able to perceive music through non-traditional senses, i.e. not using hearing. That query relates intimately with the sensation of perceiving music with the body when you are in a concert, by allowing the waves to make your body vibrate, or by an installation in which music can be hear by [hugging a tree](https://es.wikipedia.org/wiki/Archivo:Museo_Violeta_Parra_-bosque_sonoro_-arboles_musicales.jpg). So, the question I started with was: how to perceive music without hearing it? But I decided to add an accessibility component to the aforementioned question, because most music interactions are designed for people who can hear but not for deaf ones. An outstanding approach to this was made by [Myles de Bastion](https://myles.debastion.com/deafness-music), a deaf musician who makes and plays music using light as a complementary medium. So, **why don't designing a framework that allow anyone to perceive music without the necessity of hearing the sound?**

For addressing the question, I would like to take the concept [Synesthesia](https://en.wikipedia.org/wiki/Synesthesia) which is `is a perceptual phenomenon in which stimulation of one sensory or cognitive pathway leads to automatic, involuntary experiences in a second sensory or cognitive pathway.` This means that a person can have a perceptual experience in one sense due to an experience in other sense. But going further, it can be thought as a sensorial experience can be translated or interpreted from one sense to another, for example a musician who sees colors or patterns while listening to music or creating music.

Departing from the above, I will design an experience in which sound information of music can be translated into other perceptual elements, but which elements? In order to make the experience affordable and accessible as possible, I will design an application that can run in any mobile device –but in a premature stage will just be designed for OSx. Given this constrain, the senses that can be stimulated by a phone or a tablet apart from hearing are **vision** and **touch**, so how to transmit music information through these senses?

<p align="center">
  <img src="https://github.com/guillemontecinos/itp_spring_2019_music_interaction_design/blob/master/week_5/assets/sketch.jpg" align="middle" width="70%">
</p>

A technique that I feel conveys both senses is AR. By processing music I can isolate its most important features and map them to a visualization which can be displayed in the space, at the same time it is complemented with vibrations. Regarding the above, sound visualization and spatialization is a well explored field plenty of material to inspire, from **spectrograms** [1](https://www.youtube.com/watch?v=2MUZWFEEzSg), [2](https://www.youtube.com/watch?v=rqSEaF0rC60), to **abstract visualizations** [1](https://www.youtube.com/watch?v=sMHtMaym-2k), [2](https://www.youtube.com/watch?v=mEp_CtJHF0c), [3](https://www.youtube.com/watch?v=dqnMoWHon40), and covering [ML-based](https://experiments.withgoogle.com/seeing-music) music visualizing. In terms of AR sound visualizations there are interesting referents as (of course) [Zach Liebermen](https://www.instagram.com/p/Bdh_ljNAerB/), [AR Sound Vis + 3d Printing](https://www.3ders.org/articles/20131230-visualizing-active-sound-waves-with-3d-printing-augmented-reality.html) and [Wearable AR Sound Visualization](https://www.youtube.com/watch?v=sWoE7uVImD4).

### Verplank's IxD
#### DO/FEEL/KNOW
* How do I DO?
  * User experiences a physical/virtual musical space
* How do I FEEL?
  * Vision to see and touch to perceive music information via both visualization and vibration
  * FEEL: Cold because invites the user to take the device and explore the music
* How do I KNOW?
  * Turn App On -> Choose song/mic -> Choose visualization mode -> Perceive

Regarding the above, the main question is **how to keep user attention?**

#### Design framework

|IDEA|METAPHOR|MODEL|DISPLAY|
|-|-|-|-|
|A non-traditional music perceiving tool|Synesthesia|A software-based system that extracts music information and transduce it into visual and mechanical stimuli|AR app for iPhone (prototype)|

|ERROR|SCENARIO|TASK|CONTROL|
|-|-|-|-|
|There are no displays to perceive music other than hearing and deaf people is not included in design|A musician sees music in space when creating and/or hearing|Sound capture / FFT / Space Measuring / Visual and mechanical stimuli generation|Display movement in space used to see sound in space / Output modes selection|

### 7-axis dimension space diagram
<p align="center">
  <img src="https://github.com/guillemontecinos/itp_spring_2019_music_interaction_design/blob/master/week_5/assets/dimension_space_diagram.png" align="middle" width="70%">
</p>

## Week 6 - Midterm
In order to explore the possibilities of approaching the idea proposed during week 5, I decided to dive into 2D and 3D visuals generation. As a reference of sound visualization techniques I picked *Seeing sound* by Zimmerman, Mann, Kearney-Volpe, Pereira & Phillips, whilst for 3D AR sound visualization I picked Zach Lieberman's work (picture below) and *HoloDecks* by Lukasz Karluk, whilst. Additionally, I inspired in Kazimir Malevich, a Suprematist artist whose work was based on the abstraction of elemental shapes and the use of mainly warm colors.

<p align="center">
  <img src="https://github.com/guillemontecinos/itp_spring_2019_music_interaction_design/blob/master/week_6_midterm/assets/lieberman_ar.gif" align="middle" width="30%">
</p>

I split the work in 2D and 3D, in order to explore different visual aesthetics in a *simpler* context (2D) and to learn how to work with a more complex tool that allow me to display 3D graphics, as Unity (3D). The workflow consisted in analyzing the music by using a [Fast Fourier Transform](https://en.wikipedia.org/wiki/Fast_Fourier_transform) so I can track how different frequency bands evolve in time. I connected then the information of some of the frequency bands to elements in the space.

<p align="center">
  <img src="https://github.com/guillemontecinos/itp_spring_2019_music_interaction_design/blob/master/week_6_midterm/assets/workflow.png" align="middle" width="70%">
</p>

I learned how to build a 3D sound visualization in Unity, in which sound was analyzed by a 8-frequency band. By this I learned how to place a volume in the space, how to animate it with sound data. I learned basic C# coding skills as well.

<p align="center">
  <img src="https://github.com/guillemontecinos/itp_spring_2019_music_interaction_design/blob/master/week_6_midterm/assets/sound_3d_airbag.gif" align="middle" width="70%">
</p>

Finally I built a 2D animation of a Malevich painting, in order to explore how that kind of aesthetic can respond to music.

<p align="center">
  <img src="https://github.com/guillemontecinos/itp_spring_2019_music_interaction_design/blob/master/week_6_midterm/assets/sound_2d_airbag.gif" align="middle" width="50%">
</p>

I got the following notes from this process:
* FFT may not be the best tool to extract perceptual elements of music
  * Explore ML techniques
  * Other techniques
  * Beat detection
* Learn more about perception and synesthesia
  * Other ways of perceive music through shapes and color

# Week 8 - Midterm Evaluation
This is the evaluation of Ian MacDougald's *VR Composition System* midterm presentation, following the rubric developed during week 7:

## Does it work?
The prototype is currently working and was presented to the class through a video capture.

## Ants
The topic Ants addresses if the project sounds/feels like ants. I think this topic is achieved in sound terms because the instrument tends to sound like a bunch of ants talking each other. Besides, from a visual standpoint the project still looks as a bunch of purple balls, which hopefully will be improved.

## Does it engage/inspire prolonged interest?
The project seems to call user's attention –at least it called my attention– but I think it's very important that its visual representation be improved in the future. Additionally, I'd would suggest to explore other ways of interaction with the sound besides throwing the balls against the walls.

## To what extent does an arbitrary user understand how their interactions affect the system?
May be for this point it would be useful to have a description or a set of instructions at the beginning of the experience. Besides, I think it is equally interesting just to expose a user to a 3D grid of balls in a VR space and allow them to explore how to interact. It depends on the gesture you want to induce in the user.

## Does it make you feel an spatial experience?
Totally. I'd think a bit more if the walls have to be black or maybe have some texture. I'd like to see how an infinite space can deform user's perception in your project.

## Visual-side: is the "visual language" coherent, and does it accomplish the unique goals you want it to accomplish?
I think the election of this visual language is arbitrary because the project is in a prototype stage, but I'd expect a visuality that challenges the user to explore it.

## Audio-side: if the "audio-language" is as important as the visual language theoretically, in your realized system, are they truly equal?
I think in this case the audio-side of the project is as important as the visual language or even more important because is what currently sets the sensation of space.

## Midterm-Final Plan
<p align="center">
  <img src="https://github.com/guillemontecinos/itp_spring_2019_music_interaction_design/blob/master/assets/week8_midtofinal1.jpg" align="middle" width="70%">
</p>

<p align="center">
  <img src="https://github.com/guillemontecinos/itp_spring_2019_music_interaction_design/blob/master/assets/week8_midtofinal2.jpg" align="middle" width="70%">
</p>
 
 # Week 9 - Basic sound analysis and mesh rendering in oF and AR
 During the weeks after the midterm I worked in start creating 3D textures in openFrameworks. For that I followed the chapter dedicated to mesh in the oF book from which I built a 3D animated mesh based.

<p align="center">
  <img src="https://github.com/guillemontecinos/itp_spring_2019_music_interaction_design/blob/master/week_9/mesh.gif" align="middle" width="70%">
</p>

Then, I used that code as a base to place an spectrograph in a 3D space.

<p align="center">
  <img src="https://github.com/guillemontecinos/itp_spring_2019_music_interaction_design/blob/master/week_9/spectrograph.gif" align="middle" width="70%">
</p>

Finally I worked on porting the previous advances to AR. For this I finally decided to work with Android –basically because I don't need a payed license to develop prototypes. The framework consists in oF supported by Android via the Android-oF library, which in parallel the addon ofxARCore communicates between the oF script and ARCore –the Android framework that analyses the spatial motion of the device. This process was a bit tricky because there are many layers of knowledge that I still don't understand, but I'm on it. By the end I could place the mesh in the space, adn this is how it looks.

<p align="center">
  <img src="https://github.com/guillemontecinos/itp_spring_2019_music_interaction_design/blob/master/week_9/ar_mesh.gif" align="middle" width="70%">
</p>

## Week 14 - sonAR
During the last par ot the semester –specifically between week 9 and 14– the project had few changes in terms of developing platform, operative system and interaction approach. 

The first decisiont I made was to move from Android to iOS, based on the fact that even iOS is a more rigid and expensive OS to work with, it is more structured and supported, at the same time that the community around it is bigger. Since Android is Java-based and therefore more flexible to work with, it becomes an unstable framework for non-mainstream developing approaches as openFrameworks, due to Android releases are not fastly followed by the oF community that works on ofxARCore, as it happens with the coomunity around ofxARKit. Due to the above I decided to move to iOS-ARKit thanks to Matt Jacobson's support who let me borrow his iPad and Apple developer account <3.

After moving to ARKit and porting the previous mesh developments I realized that FFT analysis can't be calculated in iOS using the oF built-in function `ofSoundGetSpectrum()` since it's not implemente for mobile devices. That led me to research how to analyze sound in iOS, finding frameworks like `ofxMaxim`, `AudioKit` and the `Apple vDSP` kit. I tried Maxim with some troubles since I received the following error which seemed to be a bug on iOS 12 last release:

```
[avas] AVAudioSessionPortImpl.mm:56:ValidateRequiredFields: Unknown selected data source for Port Speaker (type: Speaker)
```

This made me think that no FFT can be calculated in iOS unless I get into Swit to code AudioKit or use vDSP in the low level, which I couldn't afford because of the project deadline. Considering this constrain I developed a simplification of the concept for the user testing, in which an element could react to a sound trigger event but no to the sound information of a track. This was a first aesthetic approach called "maraña" –the Spanish word for tangle– consisting in a floating white thread tangle that can be placed in the space and reacted to a screen touch interaction, triggering a visual animation and a drum sample. The user test meeting was very useful to solve this issue because I was adviced by my peers and Luisa to switch my ptoject a little in order to convert it into an AR sound experience in which music elements were represented physically, instead to make a real time representation of a musical piece or the ambient noise.

<p align="center">
  <img src="https://github.com/guillemontecinos/itp_spring_2019_music_interaction_design/blob/master/week_11_final/documentation/marana.gif" align="middle" width="50%">
</p>